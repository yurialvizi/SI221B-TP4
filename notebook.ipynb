{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look into the following pages to begin or find documentation on the python librairies used here:\n",
    "- http://www.python.org\n",
    "- http://scipy.org\n",
    "- http://www.numpy.org\n",
    "- http://scikit-learn.org/stable/index.html\n",
    "- http://www.loria.fr/~rougier/teaching/matplotlib/matplotlib.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "In this lab session, we will work on a practical application of supervised learning, which is arguably the simplest one: **binary supervised classification**. We will generate artificial data from two different sources, and we will try to learn a classifier that will be able to separate the data, using the **perceptron**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions and notations\n",
    "\n",
    "- $\\mathcal{X}$ is a set of *examples/observations/samples* $\\mathbf{x} = (x_1, x_2, ..., x_p) \\in \\mathbb{R}^p$. We call $x_j$ the value taken by the $j$-th variable of $\\mathbf{x}$, or its $j$-th *feature*.\n",
    "- $\\mathcal{Y}$ is the set of labels. We are in the **binary** case: there is only two possible labels. We choose to note them $\\{ -1, 1\\}$, which will make things easy by allowing us to work with the ```sign``` function.\n",
    "- $\\mathcal{D}_n = \\{(x_i, y_i), i=1, .., n\\}$ is a dataset containing $n$ examples and their labels. \n",
    "- There exists a probabilistic model governing the generation of our data given i.i.d (independent and identically distributed) random variables $X$ and $Y$: $\\forall i \\in \\{ 1, ..., n \\}, (x_i, y_i) \\sim (X, Y)$\n",
    "- We would like to build, from $\\mathcal{D}_n$, a function that we call a *classifier*, $$\\hat{f}: \\mathcal{X} \\rightarrow \\{ -1, 1 \\}$$ which for a new data point $\\mathbf{x}_{new}$ will give a label $\\hat{f}(\\mathbf{x}_{new})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'axes.facecolor': 'white',\n",
       " 'axes.edgecolor': '.15',\n",
       " 'axes.grid': False,\n",
       " 'axes.axisbelow': True,\n",
       " 'axes.labelcolor': '.15',\n",
       " 'figure.facecolor': 'white',\n",
       " 'grid.color': '.8',\n",
       " 'grid.linestyle': '-',\n",
       " 'text.color': '.15',\n",
       " 'xtick.color': '.15',\n",
       " 'ytick.color': '.15',\n",
       " 'xtick.direction': 'out',\n",
       " 'ytick.direction': 'out',\n",
       " 'lines.solid_capstyle': <CapStyle.round: 'round'>,\n",
       " 'patch.edgecolor': 'w',\n",
       " 'patch.force_edgecolor': True,\n",
       " 'image.cmap': 'rocket',\n",
       " 'font.family': ['sans-serif'],\n",
       " 'font.sans-serif': ['Arial',\n",
       "  'DejaVu Sans',\n",
       "  'Liberation Sans',\n",
       "  'Bitstream Vera Sans',\n",
       "  'sans-serif'],\n",
       " 'xtick.bottom': False,\n",
       " 'xtick.top': False,\n",
       " 'ytick.left': False,\n",
       " 'ytick.right': False,\n",
       " 'axes.spines.left': True,\n",
       " 'axes.spines.bottom': True,\n",
       " 'axes.spines.right': True,\n",
       " 'axes.spines.top': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"From a lab created on Mon Sep 23 17:50:04 2013 by baskiotis, salmon, gramfort\n",
    "Modified on Mon Nov 4 21:09:38 2019 by mozharovskyi\n",
    "Modified on Wed Feb 17 by labeau\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "\n",
    "################################################################\n",
    "# Code for displaying labeled data - no understanding needed ! #\n",
    "################################################################\n",
    "\n",
    "symlist = ['o', 'p', '*', 's', '+', 'x', 'D', 'v', '-', '^']\n",
    "\n",
    "rc('font', **{'family': 'sans-serif', 'sans-serif': ['Computer Modern Roman']})\n",
    "params = {'axes.labelsize': 12,\n",
    "          'font.size': 16,\n",
    "          'legend.fontsize': 16,\n",
    "          'text.usetex': False,\n",
    "          'figure.figsize': (8, 6)}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set_style(\"white\")\n",
    "sns.axes_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating artificial data \n",
    "\n",
    "For our first experiment, and in order to visualize what is happening, we will work with only two features (so, with $p=2$) so that we can plot the data and the classifier. \n",
    "\n",
    "1) Take a look at the function ```rand_gauss(n, mu, sigma)```: this function returns $n$ samples following the multi-dimensional normal distribution, with mean the vector $\\mu =$ ```mu```, and with the covariance matrix $\\Sigma$ being the diagonal matrix of the vector ```sigmas``` $=[ \\sigma_1, \\sigma_2]$. Hence, the matrix $$\\Sigma = \\begin{pmatrix}\n",
    "\\sigma_1&0 \\\\\n",
    "0 &\\sigma_2\n",
    "\\end{pmatrix}$$\n",
    "Now, generate different datasets from the function ```rand_bi_gauss``` function. What does the second output correspond to ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_gauss(n=100, mu=[1, 1], sigmas=[0.1, 0.1]):\n",
    "    \"\"\"Sample  points from a Gaussian variable.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : number of samples\n",
    "    mu : centered\n",
    "    sigma : standard deviation\n",
    "    \"\"\"\n",
    "    d = len(mu)\n",
    "    res = np.random.randn(n, d)\n",
    "    return np.array(mu + res * sigmas)\n",
    "\n",
    "\n",
    "def rand_bi_gauss(n1=100, n2=100, mu1=[1, 1], mu2=[-1, -1], sigmas1=[0.1, 0.1],\n",
    "                  sigmas2=[0.1, 0.1]):\n",
    "    \"\"\"Sample points from two Gaussian distributions.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n1 : number of sample from first distribution\n",
    "    n2 : number of sample from second distribution\n",
    "    mu1 : center for first distribution\n",
    "    mu2 : center for second distribution\n",
    "    sigma1: std deviation for first distribution\n",
    "    sigma2: std deviation for second distribution\n",
    "    \"\"\"\n",
    "    ex1 = rand_gauss(n1, mu1, sigmas1)\n",
    "    ex2 = rand_gauss(n2, mu2, sigmas2)\n",
    "    y = np.hstack([np.ones(n1), -1 * np.ones(n2)])\n",
    "    X = np.vstack([ex1, ex2])\n",
    "    ind = np.random.permutation(n1 + n2)\n",
    "    return X[ind, :], y[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with rand_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with rand_bi_gauss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Keep some of these datasets to use them in the rest of the lab. For each of them, save them as a numpy array with two coloumns for $X$, and as a vector for $Y$. Use the function ```plot_2d``` allowing you to visualize the data with their associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(X, y, w=None, step=50, alpha_choice=1):\n",
    "    \"\"\"2D dataset data ploting according to labels.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : data features\n",
    "    y : label vector\n",
    "    w :(optional) the separating hyperplan w\n",
    "    alpha_choice : control alpha display parameter\n",
    "    \"\"\"\n",
    "    min_tot0 = np.min(X[:, 0])\n",
    "    min_tot1 = np.min(X[:, 1])\n",
    "\n",
    "    max_tot0 = np.max(X[:, 0])\n",
    "    max_tot1 = np.max(X[:, 1])\n",
    "    delta0 = (max_tot0 - min_tot0)\n",
    "    delta1 = (max_tot1 - min_tot1)\n",
    "    labels = np.unique(y)\n",
    "    k = np.unique(y).shape[0]\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", k)\n",
    "    sns.set_palette(color_blind_list)\n",
    "    for i, label in enumerate(y):\n",
    "        label_num = np.where(labels == label)[0][0]\n",
    "        plt.scatter(X[i, 0], X[i, 1],\n",
    "                    c=np.reshape(color_blind_list[label_num], (1, -1)),\n",
    "                    s=80, marker=symlist[label_num])\n",
    "    plt.xlim([min_tot0 - delta0 / 10., max_tot0 + delta0 / 10.])\n",
    "    plt.ylim([min_tot1 - delta1 / 10., max_tot1 + delta1 / 10.])\n",
    "    if w is not None:\n",
    "        plt.plot([min_tot0, max_tot0],\n",
    "                 [min_tot0 * -w[1] / w[2] - w[0] / w[2],\n",
    "                  max_tot0 * -w[1] / w[2] - w[0] / w[2]],\n",
    "                 \"k\", alpha=alpha_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your generated data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The perceptron\n",
    "\n",
    "#### Linear classifiers (affine)\n",
    "\n",
    "A **linear classifier** is a classifier associating to each observation $x$ a label in $\\mathcal{Y}$ given its position related to an **affine hyperplane**. Each linear classifier is therefore linked to an affine hyperplan of $\\mathbb{R}^p$, which we define by its **directing vector** (or normal vector) $\\mathbf{w} = (w_0, w_1, ..., w_p) \\in \\mathbb{R}^{p+1}$. Note the supplementary dimension: it is used for what we call the *intercept* $w_0$ of the classifier (in French, *ordonnée à l'origine*). Geometrically, this allows the hyperplane to not go through the origin, and be shifted anywhere in the space. \n",
    "The hyperplane is then given by:\n",
    "$$ H_{\\mathbf{w}}=\\left\\{ \\mathbf{x} \\in \\mathbb{R}^{p} : \\hat{f}_{\\mathbf{w}}(\\mathbf{x}):=\n",
    "w_0 +\\sum_{j=1}^p w_j x_j=0 \\right\\}. $$\n",
    "\n",
    "In order to classify an observation $\\mathbf{x}$ (i.e, affect a label $1$ or $-1$, we use the function $sign(f(\\mathbf{x}))$, where the $sign$ function is defined as:\n",
    "$$ sign(x)=\n",
    "\\begin{cases}\n",
    " 1,& \\text{ if } x\\geq0,\\\\\n",
    "-1,& \\text{ if } x<0\n",
    "\\end{cases}\n",
    "\\enspace . $$\n",
    "\n",
    "Hence, the function  $\\mathbf{x} \\mapsto sign ( \\hat{f}_{\\mathbf{w}}(\\mathbf{x}))$ is the binary classifier of linear separation defined by $\\mathbf{w}$. The objective of the perceptron is to find an hyperplane which separates in the best possible way the data, into two groups. We would like to have, on each side of the hyperplane, labels separated into homogenous groups. The vector $\\mathbf{w}$ is called a **weight vector**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next questions, you should reuse the datasets you obtained previously.\n",
    "\n",
    "3) What does the linear separation given by the perceptron in dimension $p=2$ correspond to? See if you can find (visually) a good separation for your datasets. When is $\\hat{f}_{\\mathbf{w}}(\\mathbf{x})$ large? Negative? Positive? How can that function be interpreted geometrically? What does $w_0$ correspond to on the linear separation you found on your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Write the the function ```predict(x,w)``` that takes, as input, a vector $\\mathbf{x} \\in \\mathbb{R}^p$ and a weight vector $\\mathbf{w} \\in \\mathbb{R}^{p+1}$ and outputs the prediction $\\hat{f}_{\\mathbf{w}}(\\mathbf{x})$. Then, write ```predict_class(x,w)``` that outputs the predicted label $sign\\left(\\hat{f}_\\mathbf{w}(\\mathbf{x})\\right)$. Apply them to the following example, and display on the same plot your data, the hyperplane, and the two new points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w):\n",
    "    \"\"\"Prediction from a normal vector.\"\"\"\n",
    "    return # Complete here\n",
    "\n",
    "\n",
    "def predict_class(x, w):\n",
    "    \"\"\"Predict a class from at point x thanks to a normal vector.\"\"\"\n",
    "    return # Complete here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = [0, 1, 1] # Visually, seems like it would make an okay normal vector for a separating hyperplane\n",
    "x_test_1 = [-1, -1]\n",
    "x_test_2 = [1, 1]\n",
    "\n",
    "# Make predictions for these two points and display them, with the data and hyperplane. \n",
    "'''\n",
    "Complete here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron learning rule\n",
    "\n",
    "The **perceptron algorithm** consists in detecting when there is a mistake, meaning that there is a point that is misclassified, and moving $\\mathbf{w}$ towards having this point on the 'right side' of the hyperplane. The change in $\\mathbf{w}$ for a case when the point is misclassified is described by the following learning rule (Rosenblatt's):\n",
    "$$\\mathbf{w}\\leftarrow\\mathbf{w}+\\epsilon \\begin{pmatrix}1 \\\\ \\mathbf x_i \\end{pmatrix}\\cdot y_i$$\n",
    "where $\\epsilon$ is a *learning step*, which indicates how much we correct $\\mathbf{w}$. The method is iterative: we will go through all examples we have in our data and update the points accordingly. \n",
    "Then, the algorithm is as follows:\n",
    "\n",
    "- **Data**: \n",
    "    - The observations and their labels $\\mathcal{D}_n=\\{(\\mathbf{x}_i,y_i): 1\\leq i \\leq n\\}$\n",
    "    - The gradient step: $\\epsilon$\n",
    "    - The maximal number of iterations: $n_{iter}$\n",
    "\n",
    "- **Result**: \n",
    "    - $\\mathbf{w}$ \n",
    "- Randomly initialize $\\mathbf{w}$; initialize $j=0$\n",
    "- While ${j\\leq n_{\\rm iter}}$ \n",
    "    - $\\mathbf{w}\\leftarrow\\mathbf{w}$\n",
    "    - For $i=1$ to $n$:\n",
    "        - if $\\hat{f}_{\\mathbf{w}}(\\mathbf{x}) \\cdot y_i \\leq 0$\n",
    "            - $\\mathbf{w}\\leftarrow\\mathbf{w}+\\epsilon \\begin{pmatrix}1 \\\\ \\mathbf x_i \\end{pmatrix}\\cdot y_i$\n",
    "    - $j\\leftarrow j+1$\n",
    "        \n",
    "5. You have to complete the code for this procedure in the ```perceptron``` function:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x, y, eps, niter, w_ini):\n",
    "    \"\"\" Perceptron algorithm:\n",
    "        - x : Data\n",
    "        - y : label\n",
    "        - eps : learning rate\n",
    "        - niter : number of iterations\n",
    "        - w_ini : initial weight\n",
    "        \"\"\"\n",
    "    # Keep track of w at each iterations - the first one is w_ini\n",
    "    w = np.zeros((niter, w_ini.size))\n",
    "    w[0] = w_ini\n",
    "    \n",
    "    # Implement the learning loop\n",
    "    '''\n",
    "    Complete here\n",
    "    ''' \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Test the perceptron algorithm on the following parameters and look at how $\\mathbf{w}$ evolves during the iterations. \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an epsilon\n",
    "epsilon = 0.2\n",
    "# A number of iterations\n",
    "niter = 10\n",
    "# Initialize w_ini\n",
    "std_ini = 1.\n",
    "w_ini = std_ini * np.random.randn(X1.shape[1] + 1)\n",
    "\n",
    "# Use the 'perceptron' function \n",
    "'''\n",
    "Complete here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Display on the same figure the evolution of the boundaries according to the iterations. We can use the ```plot_2d``` function and its ```alpha_choice``` argument for that purpose.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a loop and the last argument of plot_2d to plot each hyperplane (at each iteration) \n",
    "# with decreasing transparency\n",
    "'''\n",
    "Complete here\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General case: cost function\n",
    "\n",
    "While the perceptron algorithm we saw works geometrically, in general, in order to measure the error associated to an entire dataset $\\mathcal{D}_n$ it is necessary to set a loss function $\\ell:\\mathbb{R}\\times\\mathcal{Y} \\mapsto \\mathbb{R}^+$ which measures the cost $h$ of an error $i$ when predicting an example.\n",
    "The cost that we want to minimize (as a function of $\\mathbf{w}$) is $\\mathbb{E} \\left[ \\ell(\\hat{f}_{\\mathbf{w}}(\\mathbf{x}),y)\\right]$, the expectation of the loss function on all the data. Three loss functions are commonly used and defined below :\n",
    "- the error percentage: $ZeroOneLoss(\\hat{f}_{\\mathbf{w}}(\\mathbf{x}), y) = |y-sign(\\hat{f}_{\\mathbf{w}}(\\mathbf{x}))|/2$,\n",
    "- the quadratic error: $MSELoss(\\hat{f}_{\\mathbf{w}}(\\mathbf{x}), y) = (y-\\hat{f}_{\\mathbf{w}}(\\mathbf{x}))^2$,\n",
    "- the hinge error (*charnière* in French): $HingeLoss(\\hat{f}_{\\mathbf{w}}(\\mathbf{x}), y) = \\max(0,1-y \\cdot \\hat{f}_{\\mathbf{w}}(\\mathbf{x}))$.\n",
    "The purpose here is to study these different loss functions. These three functions are already implemented, as well as their associated gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_one_loss(x, y, w):\n",
    "    \"\"\"0-1 loss function.\"\"\"\n",
    "    return abs(y - np.sign(predict(x, w))) / 2.\n",
    "\n",
    "def hinge_loss(x, y, w):\n",
    "    \"\"\"Hinge loss function.\"\"\"\n",
    "    return np.maximum(0., 1. - y * predict(x, w))\n",
    "\n",
    "def mse_loss(x, y, w):\n",
    "    \"\"\"Mean square error loss.\"\"\"\n",
    "    return (y - predict(x, w)) ** 2\n",
    "\n",
    "def gr_hinge_loss(x, y, w):\n",
    "    \"\"\"Sub-gradient of the loss function hingeloss.\"\"\"\n",
    "    return np.dot(-y * (hinge_loss(x, y, w) > 0.),\n",
    "                  np.hstack((np.ones((x.shape[0], 1)), x)))\n",
    "\n",
    "\n",
    "def gr_mse_loss(x, y, w):\n",
    "    \"\"\"Gradient of the least squares lost function.\"\"\"\n",
    "    return -2. * np.dot(y - predict(x, w),\n",
    "                        np.hstack((np.ones((x.shape[0], 1)), x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Suppose that $\\mathbf{x} \\in \\mathbb{R}^p$ and $y \\in \\mathbb{R}$ are fixed. What is the nature of the functions $\\mathbf{w} \\mapsto \\ell(\\hat{f}_{\\mathbf{w}}(\\mathbf{x}),y)$ for the three losses studied: constant, linear, quadratic, piecewise constant, linear by pieces, quadratic by pieces, etc. ?\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent algorithm\n",
    "\n",
    "In the general case, it is of course impossible to do an exhaustive search of the space $\\mathbb{R}^{p+1}$ where evolves $\\mathbf{w}$ to minimize the gradient of $\\mathbf{w} \\mapsto \\ell(\\hat{f}_{\\mathbf{w}}(\\mathbf{x}),y)$ in order to find the minimum cost. Moreover, we cannot observe $\\mathbb{E} \\left[ \\ell(\\hat{f}_{\\mathbf{w}}(\\mathbf{x}),y)\\right]$, so we can only try to **minimize its empirical counterpart**: $$\\sum_{i=1}^n \\ell(\\hat{f}_{\\mathbf{w}}(\\mathbf{x}_i),y_i)$$\n",
    "\n",
    "\n",
    "The **perceptron algorithm** actually consists in using a variant of the gradient descent algorithm, which is the usual method of optimizing a differentiable function. Here, at each step, the current weight is corrected in the same direction as the gradient, but in the opposite sense. The algorithm converges in the general case to a local minimum as long as $\\epsilon$, which we now call the **gradient step** is well chosen. Moreover, the minimum reached is global for convex functions.\n",
    "\n",
    "The stochastic gradient method is a variant which proposes not to directly use the gradient, which requires calculating a sum on the $n$ observations, but rather to draw (randomly ... or not) a pair $(\\mathbf{x}_i , y_i )$ on which a gradient is calculated. We can also show that this algorithm converges under certain conditions.\n",
    "\n",
    "The **perceptron algorithm** is now described as follows:\n",
    "\n",
    "- **Data**: \n",
    "    - The observations and their labels $\\mathcal{D}_n=\\{(\\mathbf{x}_i,y_i): 1\\leq i \\leq n\\}$\n",
    "    - The gradient step: $\\epsilon$\n",
    "    - The maximal number of iterations: $n_{iter}$\n",
    "\n",
    "- **Result**: \n",
    "    - $\\mathbf{w}$ \n",
    "- Randomly initialize $\\mathbf{w}$; initialize $j=0$\n",
    "- While ${j\\leq n_{\\rm iter}}$ \n",
    "    - $\\mathbf{w}\\leftarrow\\mathbf{w}$\n",
    "    - For $i=1$ to $n$:\n",
    "        - $\\mathbf{w}\\leftarrow\\mathbf{w}-\\epsilon\\nabla_\\mathbf{w}\\ell( \\hat{f}_{\\mathbf{w}}(\\mathbf{x}_i, y_i))$\n",
    "        - $j\\leftarrow j+1$\n",
    "\n",
    "The code for this procedure is to complete in the ```gradient``` function:\n",
    "\n",
    "*Note: The stochastic gradient method is also available in ```sklearn``` under the name ```SGDClassify``` (SGD is the abbreviation for Stochastic Gradient Descent). A description is given on the page: http://scikit-learn.org/stable/modules/sgd.html.*\n",
    "\n",
    "9. Simple gradient descent is also called **batch**, and consists in calculating the true gradient $\\frac{1}{n} \\sum_{i=1}^n \\nabla_{\\mathbf{w}}\\ell(\\hat{f}_{\\mathbf{w}}(\\mathbf{x}_i),y_i)$. In the stochastic case, or SGD, we draw uniformly a random example instead. Implement both these ways of selecting examples in the following function. \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, y, eps, niter, w_ini, loss_fun, gr_loss_fun, stochastic=True):\n",
    "    \"\"\" Algorithm for gradient descent:\n",
    "        - x : Data\n",
    "        - y : label\n",
    "        - eps : learning rate\n",
    "        - niter : number of iterations\n",
    "        - w_ini : initial weight\n",
    "        - loss_fun : cost function\n",
    "        - gr_loss_fun : gradient of the cost function \n",
    "        - stoch : True : implements SGD\n",
    "        \"\"\"\n",
    "    w = np.zeros((niter, w_ini.size))\n",
    "    w[0] = w_ini\n",
    "    \n",
    "    # We also keep track of the loss and initialize it \n",
    "    loss = np.zeros(niter)\n",
    "    loss[0] = loss_fun(x, y, w[0]).mean()\n",
    "    \n",
    "    for i in range(1, niter):\n",
    "        if stochastic: # Which indexes are we using in the SGD case ? \n",
    "            '''\n",
    "            Complete here\n",
    "            '''\n",
    "        else: # Which indexes are we using in the simple gradient case ? \n",
    "            '''\n",
    "            Complete here\n",
    "            '''\n",
    "        # Update the weight and compute the new loss\n",
    "        '''\n",
    "        Complete here\n",
    "        '''\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX1\u001b[49m[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X1' is not defined"
     ]
    }
   ],
   "source": [
    "print(X1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX1\u001b[49m[\u001b[38;5;241m1\u001b[39m,:])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X1' is not defined"
     ]
    }
   ],
   "source": [
    "print(X1[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. The perceptron algorithm we first saw is equivalent to this more general one, with a specific loss function. Which one? In this case, interpret the following condition: $\\hat{f}_{\\mathbf{w}}(\\mathbf{x}_i)\\cdot y_i \\leq 0$\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) We will graphically observe, with the function ```plot_gradient```, the evolution of $\\frac{1}{n} \\sum_{i=1}^n \\ell(\\hat{f}_{\\mathbf{w}}(\\mathbf{x}_i),y_i)$ according to $\\mathbf{w}$ following the steps of the algorithm. Why should the number of iterations be ```niter * len(y)``` instead of ```niter``` in the stochastic case ? \n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient(X, y, wh, cost_hist, loss_fun):\n",
    "    \"\"\" display 4 figures on how  (stochastic) gradient descent behaves\n",
    "    wh : solution history\n",
    "    cost_hist : cost history\n",
    "    loss_fun : loss function\n",
    "    \"\"\"\n",
    "    best = np.argmin(cost_hist)\n",
    "    plt.subplot(221)\n",
    "    plt.title('Data and hyperplane estimated')\n",
    "    plot_2d(X, y, wh[best, :])\n",
    "    plt.subplot(222)\n",
    "    plt.title('Projection of level line and algorithm path')\n",
    "    plot_cout(X, y, loss_fun, wh)\n",
    "    plt.subplot(223)\n",
    "    plt.title('Objective function vs iterations')\n",
    "    plt.plot(range(cost_hist.shape[0]), cost_hist)\n",
    "    plt.subplot(224, projection='3d')\n",
    "    plt.title('Level line and algorithm path')\n",
    "    plot_cout3d(X, y, loss_fun, wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#            Displaying tools for plot_gradient - no understanding needed ! #\n",
    "#############################################################################\n",
    "\n",
    "def frontiere(f, X, step=50, cmap_choice=cm.coolwarm):\n",
    "    \"\"\"Frontiere plotting for a decision function f.\"\"\"\n",
    "    min_tot0 = np.min(X[:, 0])\n",
    "    max_tot0 = np.max(X[:, 0])\n",
    "    min_tot1 = np.min(X[:, 1])\n",
    "    max_tot1 = np.max(X[:, 1])\n",
    "    delta0 = (max_tot0 - min_tot0)\n",
    "    delta1 = (max_tot1 - min_tot1)\n",
    "    xx, yy = np.meshgrid(np.arange(min_tot0, max_tot0, delta0 / step),\n",
    "                         np.arange(min_tot1, max_tot1, delta1 / step))\n",
    "    z = np.array([f(vec) for vec in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    z = z.reshape(xx.shape)\n",
    "    plt.imshow(z, origin='lower', interpolation=\"nearest\", cmap=cmap_choice,\n",
    "               extent=[min_tot0, max_tot0, min_tot1, max_tot1])\n",
    "    plt.colorbar()\n",
    "\n",
    "def frontiere_new(clf, X, y, w=None, step=50, alpha_choice=1, colorbar=True,\n",
    "                  samples=True, n_labels=3, n_neighbors=3):\n",
    "    \"\"\"Trace la frontiere pour la fonction de decision de clf.\"\"\"\n",
    "    min_tot0 = np.min(X[:, 0])\n",
    "    min_tot1 = np.min(X[:, 1])\n",
    "\n",
    "    max_tot0 = np.max(X[:, 0])\n",
    "    max_tot1 = np.max(X[:, 1])\n",
    "    delta0 = (max_tot0 - min_tot0)\n",
    "    delta1 = (max_tot1 - min_tot1)\n",
    "    xx, yy = np.meshgrid(np.arange(min_tot0, max_tot0, delta0 / step),\n",
    "                         np.arange(min_tot1, max_tot1, delta1 / step))\n",
    "    XX = np.c_[xx.ravel(), yy.ravel()]\n",
    "    print(XX.shape)\n",
    "    z = clf.predict(XX)\n",
    "    z = z.reshape(xx.shape)\n",
    "    labels = np.unique(z)\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", labels.shape[0])\n",
    "    my_cmap = ListedColormap(color_blind_list)\n",
    "    plt.imshow(z, origin='lower', interpolation=\"mitchell\", alpha=0.80,\n",
    "               cmap=my_cmap, extent=[min_tot0, max_tot0, min_tot1, max_tot1])\n",
    "    if colorbar is True:\n",
    "        ax = plt.gca()\n",
    "        cbar = plt.colorbar(ticks=labels)\n",
    "        cbar.ax.set_yticklabels(labels)\n",
    "\n",
    "    # color_blind_list = sns.color_palette(\"colorblind\", labels.shape[0])\n",
    "    # sns.set_palette(color_blind_list)\n",
    "    ax = plt.gca()\n",
    "    if samples is True:\n",
    "        for i, label in enumerate(y):\n",
    "            label_num = np.where(labels == label)[0][0]\n",
    "            plt.scatter(X[i, 0], X[i, 1], c=[color_blind_list[label_num]],\n",
    "                        s=80, marker=symlist[label_num])\n",
    "    plt.xlim([min_tot0, max_tot0])\n",
    "    plt.ylim([min_tot1, max_tot1])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    if w is not None:\n",
    "        plt.plot([min_tot0, max_tot0],\n",
    "                 [min_tot0 * -w[1] / w[2] - w[0] / w[2],\n",
    "                  max_tot0 * -w[1] / w[2] - w[0] / w[2]],\n",
    "                 \"k\", alpha=alpha_choice)\n",
    "    plt.title(\"L=\" + str(n_labels) + \",k=\" +\n",
    "              str(n_neighbors))\n",
    "\n",
    "def frontiere_3d(f, data, step=20):\n",
    "    \"\"\"Plot the 3d frontiere for the decision function f.\"\"\"\n",
    "    ax = plt.gca(projection='3d')\n",
    "    xmin, xmax = data[:, 0].min() - 1., data[:, 0].max() + 1.\n",
    "    ymin, ymax = data[:, 1].min() - 1., data[:, 1].max() + 1.\n",
    "    xx, yy = np.meshgrid(np.arange(xmin, xmax, (xmax - xmin) * 1. / step),\n",
    "                         np.arange(ymin, ymax, (ymax - ymin) * 1. / step))\n",
    "    z = np.array([f(vec) for vec in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    z = z.reshape(xx.shape)\n",
    "    ax.plot_surface(xx, yy, z, rstride=1, cstride=1,\n",
    "                    linewidth=0., antialiased=False,\n",
    "                    cmap=plt.cm.coolwarm)\n",
    "\n",
    "def plot_cout(X, y, loss_fun, w=None):\n",
    "    \"\"\"Plot the cost function encoded by loss_fun,\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : data features\n",
    "    y :  labels\n",
    "    loss_fun : loss function\n",
    "    w : (optionnal) can be used to give a historic path of the weights \"\"\"\n",
    "    def _inter(wn):\n",
    "        ww = np.zeros(3)\n",
    "        ww[1:] = wn\n",
    "        return loss_fun(X, y, ww).mean()\n",
    "    datarange = np.array([[np.min(X[:, 0]), np.min(X[:, 1])],\n",
    "                          [np.max(X[:, 0]), np.max(X[:, 1])]])\n",
    "    frontiere(_inter, np.array(datarange))\n",
    "    if w is not None:\n",
    "        plt.plot(w[:, 1], w[:, 2], 'k')\n",
    "    plt.xlim([np.min(X[:, 0]), np.max(X[:, 0])])\n",
    "    plt.ylim([np.min(X[:, 1]), np.max(X[:, 1])])\n",
    "\n",
    "def plot_cout3d(x, y, loss_fun, w):\n",
    "    \"\"\" trace le cout de la fonction cout loss_fun passee en parametre, en x,y,\n",
    "        en faisant varier les coordonnees du poids w.\n",
    "        W peut etre utilise pour passer un historique de poids\"\"\"\n",
    "    def _inter(wn):\n",
    "        ww = np.zeros(3)\n",
    "        ww[1:] = wn\n",
    "        return loss_fun(x, y, ww).mean()\n",
    "\n",
    "    datarange = np.array([[w[:, 1].min(), w[:, 2].min()],\n",
    "                         [w[:, 1].max(), w[:, 2].max()]])\n",
    "    frontiere_3d(_inter, np.array(datarange))\n",
    "    plt.plot(w[:, 1], w[:, 2], np.array([_inter(w[i, 1:]) for i in\n",
    "             range(w.shape[0])]), 'k-', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Experiment on different datasets: use either the cost functions provided here with ```gradient```,  either the ```sklearn``` function. Study performance according to the following points: the number of iterations, the cost function, the difficulty of the problem (whether the classes are easily separable or not, which you can act on by choosing different gaussian to generate from). Do you at any point observe strange behaviour ? If so, what is the reason?\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.05\n",
    "niter = 3\n",
    "plt.figure(8, figsize=(15, 15))\n",
    "plt.suptitle('MSE and stochastic')\n",
    "\n",
    "'''\n",
    "Complete here\n",
    "'''\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.05\n",
    "niter = 10\n",
    "plt.figure(8, figsize=(15, 15))\n",
    "plt.suptitle('Hinge and stochastic')\n",
    "\n",
    "'''\n",
    "Complete here\n",
    "'''\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Study as before the behavior of the gradient descent algorithm, but with the option ```stoch=True``` disabled.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.001\n",
    "niter = 3\n",
    "plt.figure(8, figsize=(15, 15))\n",
    "plt.suptitle('MSE non stochastic')\n",
    "\n",
    "'''\n",
    "Complete here\n",
    "'''\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus question: Implement the  prediction function of the probabilistic perceptron seen in class, using the given  ```sigmoid``` function. Then, implement the NLL-loss and its gradient as seen in class, and experiment with them similarly as before. \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful, this is a naïve implementation that is not numerically stable\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def predict_probability(x, w):\n",
    "    \"\"\"Predict a probability to be in class 1 from at point x thanks to a normal vector.\"\"\"\n",
    "    return # Complete here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(x, y, w):\n",
    "    \"\"\"Maximum likelihood estimation loss\"\"\"\n",
    "    y = (y+1)/2 # We need 0/1 classes instead of -1/1 classes\n",
    "    return # Complete here\n",
    "\n",
    "def gr_nll_loss(x, y, w):\n",
    "    \"\"\"Sub-gradient of the loss function hingeloss.\"\"\"\n",
    "    y = (y+1)/2\n",
    "    return # Complete here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.05\n",
    "niter = 3\n",
    "plt.figure(8, figsize=(15, 15))\n",
    "plt.suptitle('MLE and stochastic')\n",
    "\n",
    "'''\n",
    "Complete here\n",
    "'''\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from Chloé Clavel and Matthieu Labeau's lab, all questions and suggestions should be sent to maria.boritchev@telecom-paris.fr ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}